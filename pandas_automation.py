# -*- coding: utf-8 -*-
"""Copy of pandas_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U10dx4ZUnWwQ74ShCEYBuaZq0DgZqHcO

**Install packages**
"""

import nltk
import spacy
from nltk.corpus import wordnet
from spacy import displacy

"""# Load parsing routines"""
def show_chunks(text):
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(text)
  for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
          chunk.root.head.text)
def print_pic(text):
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(text)
  import webbrowser
  from pathlib import Path
  html = displacy.render(doc, style="dep")
  out_path = Path.cwd() / "dep_parse.html"
  out_path.write_text(html, encoding="utf-8")
  print(f"Wrote {out_path} ({len(html)} bytes)")
  webbrowser.open(out_path.as_uri())
def print_chunks_pic(text):
  nlp = spacy.load("en_core_web_sm")
  nlp.add_pipe("merge_noun_chunks", last=True)
  doc = nlp(text)
  import webbrowser
  from pathlib import Path
  html = displacy.render(doc, style="dep")
  out_path = Path.cwd() / "dep_chunks_parse.html"
  out_path.write_text(html, encoding="utf-8")
  print(f"Wrote {out_path} ({len(html)} bytes)")
  webbrowser.open(out_path.as_uri())
def is_equative(clause):
  nlp = spacy.load("en_core_web_sm")
  clause_tokens = nlp(clause)
  dec_made = False
  clause_attr = False
  for token in clause_tokens:
    if token.dep_ == 'attr':
      clause_attr = True
  if clause_attr == True:
    for token in clause_tokens:
      if token.dep_ == 'attr' and token.head.dep_ == 'ROOT' or token.dep_ == 'nsubj' and token.head.dep_ =='ROOT':
        for child in token.children:
          if child.dep_ == 'relcl':
            dec_made = True
          elif child.dep_ == 'prep':
            for child2 in child.children:
              if child2.dep_ == 'pobj':
                for child3 in child2.children:
                  if child3.dep_ == 'relcl':
                    dec_made = True
                  elif child3.dep_ == 'prep':
                    for child4 in child3.children:
                      if child4.dep_ == 'pobj':
                        for child5 in child4.children:
                          if child5.dep_ == 'relcl':
                            dec_made = True


  return dec_made

def rearrange_noun_phrase(noun_phrase):
#moves preposition phrase to back from front
  nlp = spacy.load("en_core_web_sm")
  noun_tokens = nlp(noun_phrase)
  return_phrase =''
  root_pos = 0
  word2add =''
  for token in noun_tokens:
    if token.head.dep_ == 'ROOT' and token.dep_ == 'pobj' and token.pos_ == 'NOUN' or token.dep_ =='ROOT' and token.i>0:
      root_pos = token.i

  if root_pos > 0:
    for pos in range(root_pos-1, len(noun_tokens)):
      return_phrase = return_phrase + ' ' + noun_tokens[pos].text
  else:
    return_phrase = noun_phrase

  if root_pos > 1:
    for pos in range(0,root_pos-1):
      if pos == 0:
        word2add = noun_tokens[pos].text.lower()
      else:
        word2add = noun_tokens[pos].text
      return_phrase = return_phrase + ' ' + word2add
  return return_phrase

# takes a noun phrase and gives a concatenation of all the prepositional phrases that follow the head noun
def get_base_simple(noun_phrase):
  nlp = spacy.load("en_core_web_sm")
  phrase_rearranged = rearrange_noun_phrase(noun_phrase)
  l_edges = set()
  r_edges = set()
  phrase = nlp(phrase_rearranged)
  the_min = 0
  the_max = 0
  return_phrase = ''
  for token in phrase:
    if token.dep_ == 'ROOT':
      for child in token.children:
        if child.dep_ == 'prep':
          for child2 in child.children:
            if child2.dep_ == 'pobj':
              l_edges.add(child2.left_edge.i)
              r_edges.add(child2.right_edge.i+1)
            elif child2.dep_ == 'prep':
              for child3 in child2.children:
                if child3.dep_ == 'pobj':
                  l_edges.add(child3.left_edge.i)
                  r_edges.add(child3.right_edge.i+1)

  if len(l_edges)>0:
    the_min = min(l_edges)

  if len(r_edges)>0:
    the_max = max(r_edges)

  if len(l_edges) > 0:
    return_phrase = phrase[the_min:the_max]
  else:
    return_phrase = phrase
  return return_phrase

# is it of the form 'the the nouns phrase is A'?
def is_encrypting(clause):
  nlp = spacy.load("en_core_web_sm")
  dec_encrypt = False
  if is_equative(clause)==True:
    clause_tokens = nlp(clause)
    for token in clause_tokens:
      if token.dep_ == 'attr' and token.head.dep_ == 'ROOT':
        for child in token.children:
          if child.dep_ == 'relcl':
            dec_encrypt = True
          elif child.dep_ == 'prep':
            for child2 in child.children:
              if child2.dep_ == 'pobj':
                for child3 in child2.children:
                  if child3.dep_ == 'relcl':
                    dec_encrypt = True
                  elif child3.dep_ == 'prep':
                    for child4 in child3.children:
                      if child4.dep_ == 'pobj':
                        for child5 in child4.children:
                          if child5.dep_ == 'relcl':
                            dec_encrypt = True

  return dec_encrypt

def get_left_noun(atoken):
  left_edges = set()
  return_left = 0
  for child in atoken.children:
    left_edges.add(child.left_edge.i)
  if len(left_edges)>0: #if token has children
    return_left = min(min(left_edges),atoken.left_edge.i)
  else: #if token has no children
    return_left = atoken.left_edge.i
  return return_left #min(left_edges)

def get_right_noun(atoken):
  right_edges = set()
  return_right = 0
  for child in atoken.children:
    right_edges.add(child.right_edge.i+1)
  if len(right_edges)>0: #if token has children
    return_right = max(max(right_edges),atoken.right_edge.i+1)
  else: #if token has no children
    return_right = atoken.right_edge.i+1
  return return_right #max(right_edges)

def get_right_noun_without_embed(atoken):
  right_edges = set()
  return_right = 0
  for child in atoken.children:
    if child.dep_ == 'prep':
      right_edges.add(child.left_edge.i)
      for child2 in child.children:
        if child2.dep_ == 'pobj':
          right_edges.add(child2.left_edge.i)
          for child3 in child2.children:
            if child3.dep_ == 'relcl':
              right_edges.add(child3.left_edge.i)
            elif child3.dep_ == 'prep':
              right_edges.add(child3.left_edge.i)
              for child4 in child3.children:
                if child4.dep_ == 'pobj':
                  right_edges.add(child4.left_edge.i)
                  for child5 in child4.children:
                    if child5.dep_ == 'relcl':
                      right_edges.add(child4.nbor().left_edge.i)

  if len(right_edges)>0: #if token has children
    return_right = max(right_edges)
  else: #if token has no children
    return_right = atoken.right_edge.i+1
  return return_right #max(right_edges)

def is_indication_clause(text):
  nlp = spacy.load("en_core_web_sm")
  clause_tokens = nlp(text)
  dec_made = False
  for token in clause_tokens:
    if token.dep_ == 'ROOT' and is_syn_with(token.text,'indicate'):
      dec_made = True
  return dec_made

def is_probability_clause(text):

  nlp = spacy.load("en_core_web_sm")
  clause_tokens = nlp(text)
  dec_made = False
  for token in clause_tokens:
    if is_syn_with(token.text,'chance') and token.head.dep_ == 'ROOT' and is_syn_with(token.head.text,'is'):
      if token.dep_ == 'attr' or token.dep_ == 'nsubj':
        dec_made = True
  return dec_made

def is_syn_with(phrase,given_phrase):
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(phrase)
  # this lemmatizes the noun
  lem_phrase = ''
  names=[]
  for token in doc:
    lem_phrase =token.lemma_
  is_prob = False
  for syn in wordnet.synsets(given_phrase):
    for lemma in syn.lemma_names():
      names.append(lemma)
  if lem_phrase in names:
    is_prob = True
  return is_prob

def get_base(text):
  nlp = spacy.load('en_core_web_sm')
  head_phrase =''
  text_tokens = nlp(text)
  left_end = 0
  embed_verb_pos = 0
  embed_subj = ''
  prop_phrase = ''
  embed_text=''
  pre_embed_text =''
  if is_indication_clause(text):
    for token in text_tokens:
      if token.head.dep_=='ROOT' and token.dep_=='ccomp':
        embed_text = ''
        embed_verb_pos = token.i
        for atoken in token.subtree:
          embed_text = embed_text + ' ' + atoken.text
    embed_text = embed_text.strip()
    return get_base(embed_text)
  elif is_probability_clause(text):
    for token in text_tokens:
      if token.dep_ == 'acl' and token.head.head.dep_ == 'ROOT':
        if token.head.dep_ == 'attr' or token.head.dep_ == 'nsubj':
          for atoken in token.subtree:
            embed_text = embed_text + ' ' + atoken.text
        return get_base(embed_text)
      elif token.dep_ == 'relcl' and is_syn_with(token.head.text,'chance') and token.head.head.dep_ =='ROOT':
        for atoken in token.subtree:
          if atoken.i > token.head.i+1 or atoken.text != 'that':
            embed_text = embed_text + ' ' + atoken.text
            embed_text = embed_text.strip()
        return embed_text

    return get_base(embed_text)
  else:
    if is_equative(text) == False:
      for token in text_tokens:   #find where the root verb is
        if token.dep_ == 'ROOT':
          left_end = token.i
      for token in text_tokens:
        if token.dep_ == 'nsubj' and token.head.dep_ == 'ROOT' or token.dep_ == 'npadvmod' and token.head.dep_ == 'ROOT':
          left_end = get_left_noun(token)
          head_phrase = text_tokens[left_end:get_right_noun(token)].text
      for token in text_tokens:
        if token.dep_ == 'prep' and token.head.dep_ =='ROOT' and token.i < left_end:
          prop_phrase = text_tokens[token.left_edge.i:token.right_edge.i+1].text
          head_phrase = prop_phrase + ' ' + head_phrase
    else:
      the_dep = 'attr' #for encrypting clauses
      if is_encrypting(text)!=True:
        the_dep = 'nsubj'  #for non-encrypting clauses
      for token in text_tokens:
        right_edge_index = 0
        if token.dep_ == the_dep and token.head.dep_ == 'ROOT':
          if there_is_embedding(token):
            right_edge_index = get_right_noun_without_embed(token)
          else:
            right_edge_index = token.right_edge.i+1
          head_phrase = text_tokens[get_left_noun(token):right_edge_index].text
    return get_base_simple(head_phrase).text
  

#text = '27% is the proportion of people aged 31-40 years that tested positive for an illicit drug.'
#nlp = spacy.load('en_core_web_sm')
#text_tokens = nlp(text)
#for token in text_tokens:
#  if token.dep_ == 'attr' and token.text == 'proportion':
#    print(token.text)
#    for child in token.children:
#      print(child.text)

#check that the token has an embedded clause
def there_is_embedding(thetoken):
  the_decision = False
  for descendent in thetoken.subtree:
    if descendent.head != thetoken and descendent.dep_ == 'relcl':
      the_decision = True
  return the_decision
